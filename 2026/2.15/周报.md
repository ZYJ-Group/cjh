# 周报  

## CLIP

![1](C:\Users\70269\Desktop\周报\2026\2.4\1.png)

#### 1. 模型架构：双编码器结构

CLIP 包含两个核心编码器，协同学习跨模态嵌入空间：

图像编码器，支持两种架构

- 改进型 ResNet：基于 ResNet-50/101，加入 ResNetD 优化、抗锯齿模糊池化，替换全局平均池化为注意力池化（单层多头 QKV 注意力，查询基于图像全局特征）。
- Vision Transformer（ViT）：遵循 ViT 原始设计，仅在 patch + 位置嵌入后增加一层归一化，调整初始化策略。

文本编码器，Transformer 架构

- 基础配置：12 层、512 维隐藏层、8 个注意力头，6300 万参数。
- 文本处理：采用字节对编码（BPE），词汇量 49152，最大序列长度 76，以 [SOS] 和 [EOS] 标记包裹文本，取 [EOS] 位置的高层特征作为文本表征。

#### 2. 数据集构建：WIT（WebImageText）

- 规模：4 亿个（图像 - 文本）对，来自互联网公开来源，覆盖极广的视觉概念。
- 构建逻辑：通过 50 万个查询词（基于维基百科高频词、双词组合、WordNet 同义词集）搜索图像，每个查询词最多保留 2 万个样本，保证数据多样性与平衡性。
- 过滤标准：仅保留含英文自然语言标题 / 描述的图像，剔除自动生成文件名、相机参数等无意义文本的样本。

#### 3. 预训练目标：对比学习（InfoNCE 变体）

核心任务：让模型判断 “图像与文本是否为配对样本”，而非预测文本具体单词，大幅提升训练效率。

- 训练逻辑：

  1. 输入一批（N 个）图像和对应的 N 个文本，形成 N×N 个可能的（图像 - 文本）配对。
  2. 模型需最大化 “真实配对” 的余弦相似度，最小化 “虚假配对” 的相似度。
  3. 损失函数：对称交叉熵损失（同时计算 “图像→文本” 和 “文本→图像” 两个方向的分类损失，取平均）。

  

- 关键优化：

  - 温度参数 τ：作为可学习参数（初始值 0.07），避免 logits 过大导致训练不稳定，上限设为 100。
  - 简化设计：移除非线性投影层，直接用线性投影将图像 / 文本表征映射到同一嵌入空间；仅使用随机裁剪作为数据增强，避免复杂变换引入噪声。

  

#### 4. 零 - shot 推理：文本生成分类器

利用预训练的跨模态对齐能力，无需任务特定训练，直接通过文本描述构建分类器：

- 推理步骤：

  1. 针对目标任务的每个类别，用文本模板（如 “A photo of a {label}.”）生成类别描述。
  2. 文本编码器将所有类别描述编码为嵌入向量，形成 “零 - shot 分类器权重”。
  3. 图像编码器编码输入图像，计算图像嵌入与所有类别嵌入的余弦相似度，经 softmax 得到预测概率。

  

- 性能优化：

  - 提示工程（Prompt Engineering）：根据任务定制模板（如细分类任务添加 “a type of pet”，OCR 任务添加引号），弥补文本分布差异。
  - 集成策略（Ensembling）：用多个不同模板生成分类器，在嵌入空间平均权重，进一步提升鲁棒性。

![48b1446c409c92ceb763e2f13655a799](C:\Users\70269\Desktop\周报\2026\2.4\48b1446c409c92ceb763e2f13655a799.jpg)

```python
image = preprocess(Image.open("/home/lcwt/SSD1/cjh/CLIP-main/2.jpg")).unsqueeze(0).to(device)
text = clip.tokenize(["a cat", "a dog", "a diagram"]).to(device)
```

![e89aeeb7f0b2e1840076ace73174105a](C:\Users\70269\Desktop\周报\2026\2.4\e89aeeb7f0b2e1840076ace73174105a.png)
