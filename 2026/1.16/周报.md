# 周报  

## 1.UVCGAN v2:用于非配对图像到图像转换的改进循环一致GAN

代码开源

**贡献**:
1.引入了一种用于图像翻译的新型混合神经架构，结合了ViT和样式调制卷积块的优点。
2.用专门的头部增强通用I2I鉴别器架构，以防止模式崩溃问题。

![1](https://github.com/ZYJ-Group/cjh/blob/main/2026/1.16/1.png)

（一）图 A：带风格调制卷积的 U-Net 主体

U-Net 是生成器的基础骨架，分为编码（Encoding）和解码（Decoding）两大路径，核心作用是 “提取图像特征→恢复图像分辨率”：

1. 编码路径（左侧 “Encoding”）

   由基础块（B₁-B₄）和下采样块（D₁-D₄）组成，逐步将输入图像下采样，提取多尺度局部特征（从纹理到语义）。

   每一步下采样后特征图通道数增加、尺寸缩小，为后续全局建模提供紧凑的特征表示。

2. 解码路径（右侧 “Decoding”）

   由上采样块（U₁-U₄）和风格调制卷积块（M₁-M₄）组成，逐步将低分辨率特征图上采样恢复至原始尺寸。

   关键创新是**风格调制卷积（Modulated Convolution）**：每个卷积层的权重会被 eViT 输出的 “风格向量” 动态调整，实现 “源图像引导的风格适配”（比如把男性人脸的 “硬朗风格” 调整为女性的 “柔和风格”）。

3. 跳接连接

   编码路径的中间特征图会直接拼接（Concatenate）到解码路径的对应层，保留细节信息，避免上采样时丢失纹理（如面部五官、背景细节）。

（二）图 B：扩展视觉 Transformer（eViT）瓶颈层

eViT 是生成器的 “全局特征建模核心”，位于 U-Net 的瓶颈层（编码路径末端、解码路径前端），核心作用是 “捕捉全局依赖 + 推断目标风格”：

1. **输入处理**：编码路径输出的高维特征图（如 16×16×C）会被展平为序列令牌（Token），并添加**位置编码（Positional Embedding）**，保留空间信息（Transformer 本身无位置感知能力）。

2. 风格令牌（Style Token）

   在令牌序列中额外添加一个可学习的 “风格令牌（S）”，其作用类似 ViT 的 [Class] 令牌，但专门用于 “风格建模”。

   该令牌与图像令牌一起输入 Transformer 编码器（n 个编码器块串联），通过自注意力机制捕捉全局特征关联后，输出 “latent 风格向量”—— 这是 UVCGAN v2 的核心创新，风格完全由源图像推断，而非随机生成（区别于 StyleGAN）。

3. 输出衔接

   eViT 的输出分为两部分：一是经过全局建模的特征令牌（用于解码路径恢复图像）；二是风格令牌 S 经过线性层转换后的风格向量（用于调制解码路径的卷积层权重）。

![2](https://github.com/ZYJ-Group/cjh/blob/main/2026/1.16/2.png)

**批统计感知鉴别器（Batch-Statistics-aware Discriminator）结构**，其核心是通过 “基础判别主体 + 批统计专用头 + 历史特征缓存” 的复合设计，在不增加 GPU 内存压力的前提下缓解 GAN 训练的模式崩溃问题。具体来看，输入的单张图像首先送入 PatchGAN 作为鉴别器主体，提取图像局部特征并输出特征向量 z；随后将 z 与 “z 缓存”（存储历史真实 / 生成图像特征的 FIFO 缓存）沿批次维度拼接，模拟大批次数据的统计信息；拼接后的特征进入批头（batch head），先通过批统计层（可选批归一化 BN 或批标准差 BSD）捕捉全局分布特征，再经 3×3 卷积和 ReLU 激活进一步优化判别信号，最终辅助鉴别器同时实现 “局部真假判断” 与 “全局分布一致性约束”。

增加一个像素级一致性损失(Pixel-wise Consistency Loss)：

![3](https://github.com/ZYJ-Group/cjh/blob/main/2026/1.16/3.png)

该术语捕获源图像和翻译图像的缩小版本之间的L1差异。其中F是将大小调整到32 ×32像素(低通滤波器)的运算符。

实验结果：

![4](https://github.com/ZYJ-Group/cjh/blob/main/2026/1.16/4.png)

