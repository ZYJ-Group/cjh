# 周报  

# TokenFlow:用于多模态理解和生成的统一图像标记器

## 一、背景

在多模态（视觉 - 语言）领域：

1.多模态理解任务（如视觉问答、图像推理）依赖视觉编码器提取高层语义特征，需强语义关联性；

2.视觉生成任务（如图像重建、文本到图像合成）依赖像素级细节编码，需精准的空间结构与纹理信息。

现有统一方案存在关键缺陷：

1.单一向量量化（VQ）编码器：仅优化重建精度，缺乏语义表达能力，导致理解任务性能下降；

2.双编码器分离设计（如 Janus）：增加模型复杂度，未解决特征表示的本质差异；

3.离散视觉输入性能瓶颈：此前离散图像 token 无法在理解任务中超越连续特征（如 CLIP）的强基线（如 LLaVA-1.5 13B）。

TokenFlow 的核心目标：设计一种统一图像分词器，同时满足理解任务的语义需求与生成任务的像素级需求，打破两者的性能权衡。

## 二、核心创新：双码本 + 共享映射架构

TokenFlow 的核心突破是**语义与像素特征学习，同时通过共享映射维持对齐**，关键设计包括三部分：

![1](C:\Users\70269\Desktop\周报\2026\1.23\1.png)

### 1. 双编码器

语义编码器（Esem）：基于预训练视觉 - 语言模型（如 CLIP ViT-B/14、SigLIP）初始化，学习与文本对齐的高层语义特征，为理解任务提供强语义先验；

像素编码器（Epix）：专注于提取低层级像素细节、空间结构与纹理信息，保障生成任务的精细度。

### 2. 双码本与共享映射机制

双码本：语义码本（Zsem）存储语义特征向量，像素码本（Zpix）存储像素特征向量，两者共享同一套索引（K 为码本大小）；

**量化策略**：通过最小化语义距离与像素距离的加权和选择最优索引，公式如下：

![2](C:\Users\70269\Desktop\周报\2026\1.23\2.png)

最优量化指标是通过最小化这两个距离的加权和来确定的。

重建损失公式为:

![3](C:\Users\70269\Desktop\周报\2026\1.23\3.png)

L2损失：保障 “基础像素一致性”（兜底）；

感知损失：提升 “视觉感知相似度”（贴合人类判断）；

对抗损失：增强 “真实感和细节丰富度”（拔高质量）；
$$
像素解码器\hat{x} = D_{\text{pix}}(z)
$$
l2表示像素级重建损失，Lp表示使 用LPIPS的感知损失，LG表示以λG为权重系数的对抗性损失。

LPIPS 的完整计算过程包含 “特征提取→通道加权→距离计算→多尺度融合” 四步

![4](C:\Users\70269\Desktop\周报\2026\1.23\4.png)

Fl(⋅)：预训练网络第l层的特征提取函数，输出特征图维度为Cl×Hl×Wl（Cl为通道数，Hl为高度，Wl为宽度）；

Wl：第l层的通道权重矩阵（维度Cl×1×1），由 LPIPS 在预训练阶段学习得到，用于调整不同通道的重要性；

∥⋅∥22：L2 范数的平方（欧氏距离平方），衡量加权后真实与生成特征的差异；

系数ClHlWl1：对特征图所有元素求和后取平均，避免因特征尺寸过大导致损失值异常。

多尺度融合：

![5](C:\Users\70269\Desktop\周报\2026\1.23\5.png)

### 3. 多尺度向量量化（MSVQ）与训练目标

多尺度结构：采用多层级量化，捕捉不同粒度的视觉特征，为生成任务的 “下一级预测” 提供基础；

训练损失：总损失包含三部分：

​	语义损失（Lsem）：解码语义特征与教师模型（如 CLIP）提取特征的 L2 距离；

​	像素损失（Lpix）：包含像素级重建损失、感知损失（LPIPS）与对抗损失，保障生成质量；

​	量化损失（LVQ）：含承诺损失（commitment loss），确保编码器与码本的对齐。

## 三、实验

多模态基准评价

![6](C:\Users\70269\Desktop\周报\2026\1.23\6.png)

消融

![7](C:\Users\70269\Desktop\周报\2026\1.23\7.png)

![8](C:\Users\70269\Desktop\周报\2026\1.23\8.png)

重建后的图像保留了语义内容，但高频细节明显丢失。







上周看的论文，用他的开源代码跑我的数据集只能生成一张纯黑图片没有效果，然后项目的文档和注释都很少，应该是跑不起来了。
