# 周报  

## 1.go语言的学习情况  

本周把go语言装好了，下周开始学习go语言。

# 2.论文  

本周运行了SAM的代码，看了一点SAM的论文，下周把论文看完。

左侧为原图，右侧为使用vit_h模型分割mask后的图片

![test](C:\Users\70269\Desktop\周报\2025\7.14\test.jpg)![result_with_masks](C:\Users\70269\Desktop\周报\2025\7.14\result_with_masks.jpg)

论文中有三个子模块

提示分割模块：

![1](C:\Users\70269\Desktop\周报\2025\7.14\1.png)

给定一张图片和给定对应的提示信息，得到对应位置的mask区域。这里的提示信息可以是mask、points、box或者是text，如果是mask的话则使用卷积来进行表示，如果是其余的三种提示词则使用位置编码的形式来进行表示，其中text可以通过clip一类的模型获取词嵌入。其中主干网络的部分使用的特征提取能力更强的VIT网络。

SAM模块：

![2](C:\Users\70269\Desktop\周报\2025\7.14\2.png)

一个强大的图像编码器计算图像嵌入，一个提示编码器嵌入提示，然后将这两个信息源在一个轻量级的掩码解码器中结合起来，预测分割掩码。我们专注于点、框和掩码提示，并展示了使用自由形式文本提示的初步结果。为了使SAM具备处理歧义的能力，我们设计它针对单个提示预测多个掩码，从而使SAM能够自然地处理歧义，例如衬衫与人之间的示例。

图像编码器：出于可扩展性和强大的预训练方法的考虑，我们使用了一个经过最小调整以适应高分辨率输入的MAE预训练视觉Transformer（ViT）。图像编码器每张图像运行一次，并可在提示模型之前应用，这里使用的mae来进行预训练。

提示编码器：我们考虑两组提示，稀疏提示（点、框、文本）和密集提示（掩码）。我们用位置编码来表示点和框，并将其与每种提示类型的学习嵌入和来自CLIP的现成文本编码器中的自由格式文本相加。密集提示（即掩码）使用卷积进行嵌入，并与图像嵌入进行逐元素相加。

掩码的解码器：掩码解码器能够高效地将图像嵌入、提示嵌入和输出标记映射到一个掩码。采用了一个经过修改的Transformer解码器块，后面跟着一个动态掩码预测头。我们修改后的解码器块在两个方向上（从提示到图像嵌入和从图像嵌入到提示）使用提示自注意力和交叉注意力来更新所有嵌入。运行两个块之后，我们对图像嵌入进行上采样，并且一个多层感知机（MLP）将输出标记映射到一个动态线性分类器，然后该分类器计算图像每个位置的前景掩码概率。

数据引擎模块：

![3](C:\Users\70269\Desktop\周报\2025\7.14\3.png)

我们与模型内循环数据集标注（见图1c）共同开发我们的模型。我们的数据引擎有三个阶段：辅助手动、半自动和全自动。在第一阶段，SAM辅助标注者标注掩码，类似于经典的交互式分割设置。在第二阶段，SAM可以通过提示可能的对象位置自动为一部分对象生成掩码，而标注者则专注于标注剩余的对象，这有助于增加掩码的多样性。在最后阶段，我们使用前景点的常规网格提示SAM，平均每张图像生成约100个高质量掩码。



