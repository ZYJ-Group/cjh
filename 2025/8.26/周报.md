# 周报  

## 1.go语言的学习情况  

本周在学gin框架。

## 2.SAM2论文

​	本文提出了 Segment Anything Model 2 (SAM 2)，这是解决图像和视频中可提示视觉分割问题的基础模型。本文建立了一个数据引擎，通过用户互动改进模型和数据，从而收集到迄今为止最大的视频分割数据集。本文的模型是一个简单的transformer架构，具有流式内存，可用于实时视频处理。在本文的数据基础上训练的 SAM 2 在各种任务中都表现出色。在视频分割方面，本文观察到了更高的准确性，使用的交互次数比之前的方法少 3 倍。在图像分割中，本文的模型比 Segment Anything Model (SAM) 更准确，速度快 6 倍。

**引言**

​	视频分割的目的是确定实体的时空范围，这提出了比图像分割更独特的挑战。由于运动、变形、遮挡、光照变化和其他因素，实体的外观会发生显著变化。由于摄像机运动、模糊和分辨率较低，视频的质量往往低于图像。此外，高效处理大量帧也是一项关键挑战。虽然 SAM 成功地解决了图像分割问题，但现有的视频分割模型和数据集却无法提供 “分割视频中任何内容 ”的类似能力。本文提出Segment Anything Model 2 (SAM 2),一个视频和图像分割的统一模型（将图像视为单帧视频），如下图所示：

 ![1](C:\Users\70269\Desktop\周报\2025\8.26\1.png)

 

​	本文将重点放在可提示视觉分割（PVS）任务上，该任务将图像分割推广到视频领域。该任务将视频任意帧上的点、框或掩码作为输入，以定义感兴趣的片段，并预测其时空掩码（即 “小掩码”）。一旦预测出小掩码，就可以通过在其他帧中提供提示对其进行迭代改进。

​	SAM 2 配备了一个存储器，可存储对象信息和之前的交互信息，这使它能够在整个视频中生成小掩码预测，并根据存储在存储器中的以前观察到的帧的对象上下文有效地修正这些预测。本文的流式结构是将 SAM 自然地推广到视频领域，一次处理一个视频帧，并配备一个记忆关注模块，以关注目标对象的先前记忆。当应用于图像时，内存是空的，模型的行为与 SAM 相似。这个存储器应该是借鉴了半监督VOS里面的设计。

​	本文还采用了一个数据引擎，通过使用本文的模型与注释者互动注释新的和具有挑战性的数据来生成训练数据。与大多数现有的视频分割数据集不同，我们的数据引擎不局限于特定类别的对象，而是旨在为分割任何具有有效边界的对象提供训练数据，包括部件和子部件。

​	本文提出的视频分割（SA-V）数据集包含 50.9K 个视频中的 3550 万个掩码，比现有视频分割数据集多 53 倍。SA-V 具有挑战性，因为视频中的小物体和部分会被遮挡并重新出现。SA-V 数据集具有地域多样性，SAM 2 的公平性评估表明，基于感知性别的视频分割性能差异极小，评估的三个感知年龄组之间的差异也很小。许多zero-shot基准测试（包括 17 个视频分割基准测试和 37 个单图像分割基准测试）表明，SAM 2 在各种视频和图像分布中都很有效。

**模型整体介绍**

 ![2](C:\Users\70269\Desktop\周报\2025\8.26\2.png)

​	其流程如上图所示，对于给定的帧，分割预测取决于当前提示和/或先前观察到的记忆。视频以流媒体的方式处理，图像编码器每次使用一个帧，并从前一帧中交叉关注目标对象的记忆。掩码解码器(也可选择接受输入提示)预测该帧的分段掩码。最后，存储器编码器转换预测和图像编码器嵌入(未在图中显示)，以便在未来帧中使用。

​	SAM 2支持单个帧上的点、框和掩码提示，以定义要在视频中分割的对象的空间范围。对于图像输入，模型的行为类似于SAM。可提示的轻量级掩码解码器接受帧嵌入并提示(如果有的话)当前帧并输出该帧的分段掩码。可以在框架上迭代地添加提示，以改进掩码。

​	与SAM不同，SAM 2解码器使用的帧嵌入不是直接来自图像编码器，而是以过去预测和提示帧的记忆为条件。相对于当前帧提示帧也可能“来自未来”。帧的memory由存储器编码器根据当前的预测产生，并放置在memory bank中以供以后的帧使用。内存attention操作从图像编码器获取每帧嵌入，并在memory bank上对其进行调整，以产生之后传递给掩码解码器的嵌入。
